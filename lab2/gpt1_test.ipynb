{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "529509be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x11ed91810>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import gpt1\n",
    "\n",
    "torch.manual_seed(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e8490bb",
   "metadata": {},
   "source": [
    "# GPT\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "43777a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cpu'\n",
    "batch_size = 2\n",
    "seq_len = 40\n",
    "\n",
    "max_seq_len = 300\n",
    "vocab_size = 100\n",
    "emb_size = 128\n",
    "\n",
    "head_size = 4\n",
    "num_heads = 3\n",
    "num_layers = 5\n",
    "dropout = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "da23482f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 40]), torch.int64)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# на входе тензор токенов (batch_size,seq_len)\n",
    "x = torch.randint(low=0,high=vocab_size-1,size=(batch_size,seq_len), dtype=torch.long, device=device)\n",
    "x.shape, x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "596180f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt = gpt1.GPT(\n",
    "    vocab_size=vocab_size,\n",
    "    max_seq_len=max_seq_len,\n",
    "    emb_size=emb_size,\n",
    "    num_heads=num_heads,\n",
    "    head_size=head_size,\n",
    "    num_layers=num_layers,\n",
    "    dropout=dropout,\n",
    "    device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "38eb0bfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 40, 100]), torch.float32, False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  на выходе loggits batch_size x seq_len x vocab_size\n",
    "y = gpt.forward(x)\n",
    "y.shape, y.dtype, torch.isnan(y).any().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2e66be",
   "metadata": {},
   "source": [
    "# GPT.generate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dbe012dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 60]),\n",
       " torch.int64,\n",
       " False,\n",
       " tensor([ 1,  4,  6,  7,  8,  9, 10, 11, 12, 13, 15, 17, 18, 20, 21, 22, 23, 24,\n",
       "         25, 26, 27, 31, 32, 33, 34, 35, 40, 41, 42, 43, 45, 47, 48, 49, 50, 52,\n",
       "         53, 54, 55, 57, 58, 60, 62, 63, 65, 67, 68, 69, 70, 72, 74, 75, 76, 79,\n",
       "         80, 81, 82, 83, 84, 85, 89, 91, 94, 95, 96, 98]),\n",
       " 66)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_new_tokens = 20\n",
    "#  на выходе []int batch_size x (seq_len + max_new_tokens)\n",
    "new_seq = gpt.generate(x=x,max_new_tokens=max_new_tokens)\n",
    "new_seq.shape, new_seq.dtype, torch.isnan(new_seq).any().item(), torch.unique(new_seq), len(torch.unique(new_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a4363f13",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 4, 3]), torch.float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# заготовка, чтоб посмотреть как работает softmax\n",
    "# batch_size x seq_len x vocab_size\n",
    "logits = torch.rand([2,4,3])\n",
    "logits.shape,logits.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4ec3ecbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 3]),\n",
       " tensor([[0.7192, 0.8887, 0.5660],\n",
       "         [0.4660, 0.9185, 0.3428]]))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# берём последний токен\n",
    "last_log = logits[:,-1,:]\n",
    "last_log.shape, last_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2071c218",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3286, 0.3894, 0.2820],\n",
       "        [0.2893, 0.4549, 0.2558]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# сумма по каждой строке 1\n",
    "# для каждого batch один следующий токен\n",
    "prob = torch.softmax(last_log,dim=-1)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b49c6e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "0.5210+ 0.2427+ 0.2363"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c6415739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1],\n",
       "         [1]]),\n",
       " torch.Size([2, 1]))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# keep_dim чтоб потом можно было сделать cat c batch_size x seq_len -> batch_size x (seq_len+1)\n",
    "next_token = torch.argmax(prob,dim=-1,keepdim=True)\n",
    "next_token, next_token.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5747011",
   "metadata": {},
   "source": [
    "# GPT.generate (multinominal sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc8b78b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 60]),\n",
       " torch.int64,\n",
       " False,\n",
       " tensor([ 1,  3,  4,  5,  7,  8,  9, 10, 11, 12, 13, 15, 18, 19, 20, 21, 22, 23,\n",
       "         24, 25, 26, 27, 29, 32, 33, 35, 36, 39, 40, 41, 42, 44, 45, 46, 47, 48,\n",
       "         49, 50, 52, 53, 54, 55, 57, 58, 60, 62, 64, 65, 67, 68, 69, 74, 76, 79,\n",
       "         80, 81, 82, 83, 84, 89, 94, 95]),\n",
       " 62)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# включили сэмплирование - набор токенов в ответе стал разнообразней (не только самые вероятные)\n",
    "new_seq = gpt.generate(x=x,max_new_tokens=max_new_tokens,do_sample=True)\n",
    "new_seq.shape, new_seq.dtype, torch.isnan(new_seq).any().item(), torch.unique(new_seq),len(torch.unique(new_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9fae3af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3286, 0.3894, 0.2820],\n",
       "        [0.2893, 0.4549, 0.2558]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# заготовка для Multinomial sampling (вероятностного сэмплирования)\n",
    "prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a12d837b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[2],\n",
       "         [1]]),\n",
       " torch.Size([2, 1]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# здесь с размерностью всё хорошо batch_size x 1\n",
    "next_token=torch.multinomial(prob,num_samples=1)\n",
    "next_token,next_token.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca57cb1",
   "metadata": {},
   "source": [
    "# GPT.generate (temperature)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "37758e1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 60]),\n",
       " torch.int64,\n",
       " False,\n",
       " tensor([ 1,  4,  6,  7,  9, 10, 11, 12, 13, 15, 17, 18, 20, 21, 22, 23, 24, 25,\n",
       "         26, 27, 29, 32, 33, 34, 35, 37, 40, 41, 42, 47, 48, 49, 50, 52, 53, 54,\n",
       "         55, 57, 58, 60, 62, 63, 65, 66, 67, 68, 69, 70, 74, 79, 80, 81, 82, 83,\n",
       "         84, 85, 87, 89, 90, 93, 94, 95, 96, 98]),\n",
       " 64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# с температурой можем получить промежуточный результат\n",
    "new_seq = gpt.generate(x=x,max_new_tokens=max_new_tokens,do_sample=True, temperature=0.3)\n",
    "new_seq.shape, new_seq.dtype, torch.isnan(new_seq).any().item(), torch.unique(new_seq), len(torch.unique(new_seq))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1331435f",
   "metadata": {},
   "source": [
    "# GPT.generate (top_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a2243cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 60]),\n",
       " torch.int64,\n",
       " False,\n",
       " tensor([ 1,  2,  4,  6,  7,  8,  9, 10, 11, 12, 13, 15, 17, 18, 20, 21, 22, 23,\n",
       "         24, 25, 26, 27, 29, 30, 31, 32, 33, 34, 35, 40, 41, 42, 43, 45, 46, 47,\n",
       "         48, 49, 50, 52, 53, 54, 55, 57, 58, 60, 62, 65, 67, 68, 69, 70, 71, 74,\n",
       "         79, 80, 81, 82, 83, 84, 85, 89, 91, 93, 94, 95, 96]),\n",
       " 67)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# не всё так однозначно - кол-во различных токенов может и нерасти растёт с ростом top_k\n",
    "new_seq = gpt.generate(x=x,max_new_tokens=max_new_tokens,do_sample=True, top_k=2)\n",
    "new_seq.shape, new_seq.dtype, torch.isnan(new_seq).any().item(), torch.unique(new_seq), len(torch.unique(new_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fe9e9d66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 60]),\n",
       " torch.int64,\n",
       " False,\n",
       " tensor([ 0,  1,  2,  3,  4,  5,  7,  8,  9, 10, 11, 12, 13, 15, 18, 20, 21, 22,\n",
       "         23, 24, 25, 26, 27, 29, 32, 33, 35, 40, 41, 42, 46, 47, 48, 49, 50, 51,\n",
       "         52, 53, 54, 55, 57, 58, 59, 60, 62, 65, 66, 67, 68, 69, 70, 71, 72, 74,\n",
       "         75, 79, 80, 81, 82, 83, 84, 86, 89, 90, 94, 95, 98]),\n",
       " 67)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_seq = gpt.generate(x=x,max_new_tokens=max_new_tokens,do_sample=True, top_k=25)\n",
    "new_seq.shape, new_seq.dtype, torch.isnan(new_seq).any().item(), torch.unique(new_seq), len(torch.unique(new_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ea3db4cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]),\n",
       " torch.float32,\n",
       " tensor([[0.5369, 0.8912, 0.2275, 0.7969, 0.5160],\n",
       "         [0.5575, 0.4103, 0.9600, 0.8359, 0.5377]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# заготовка\n",
    "# batch_size x seq_len x vocab_size\n",
    "logits = torch.rand([2,4,5])\n",
    "# batch_size x vocab_size\n",
    "logits = logits[:,-1,:]\n",
    "logits.shape,logits.dtype,logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c0b53f9e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1, 3, 0],\n",
       "         [2, 3, 0]]),\n",
       " torch.Size([2, 3]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# индексы трёх максимальных токенов\n",
    "_, top_k_ind = torch.topk(logits,k=3, dim=-1)\n",
    "top_k_ind, top_k_ind.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "03e7a3bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2275, 0.5369,   -inf, 0.8912,   -inf],\n",
       "        [0.9600,   -inf, 0.5575, 0.4103,   -inf]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered = torch.full_like(logits,float('-inf'))\n",
    "filtered.scatter_(dim=-1,index=top_k_ind, src=logits)\n",
    "filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c799e227",
   "metadata": {},
   "source": [
    "# GPT.generate (top_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b7fdc236",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 60]),\n",
       " torch.int64,\n",
       " False,\n",
       " tensor([ 1,  2,  3,  4,  7,  8,  9, 10, 11, 12, 13, 14, 15, 18, 19, 20, 21, 22,\n",
       "         23, 24, 25, 26, 27, 30, 32, 33, 34, 35, 40, 41, 42, 47, 48, 49, 50, 52,\n",
       "         53, 54, 55, 56, 57, 58, 60, 62, 65, 67, 68, 69, 71, 73, 74, 77, 79, 80,\n",
       "         81, 82, 83, 84, 85, 88, 89, 90, 91, 94, 95, 98]),\n",
       " 66)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_seq = gpt.generate(x=x,max_new_tokens=max_new_tokens,do_sample=True, top_p=0.8)\n",
    "new_seq.shape, new_seq.dtype, torch.isnan(new_seq).any().item(), torch.unique(new_seq), len(torch.unique(new_seq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "adda9fd9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([2, 5]),\n",
       " torch.float32,\n",
       " tensor([[0.9162, 0.5494, 0.5673, 0.0287, 0.5526],\n",
       "         [0.6475, 0.9534, 0.4948, 0.2036, 0.8602]]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# заготовка\n",
    "# batch_size x seq_len x vocab_size\n",
    "logits = torch.rand([2,4,5])\n",
    "# batch_size x vocab_size\n",
    "logits = logits[:,-1,:]\n",
    "logits.shape,logits.dtype,logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f87783bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2853, 0.1977, 0.2013, 0.1174, 0.1983],\n",
       "         [0.1963, 0.2665, 0.1685, 0.1259, 0.2428]]),\n",
       " tensor([[0, 2, 4, 1, 3],\n",
       "         [1, 4, 0, 2, 3]]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob = torch.softmax(logits, dim=-1)\n",
    "sorted_prob, sorted_prob_ind = torch.sort(prob, descending=True, dim=-1)\n",
    "prob,sorted_prob_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e70e4f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2853, 0.2013, 0.1983, 0.1977, 0.1174],\n",
       "         [0.2665, 0.2428, 0.1963, 0.1685, 0.1259]]),\n",
       " tensor([[0.2853, 0.4865, 0.6849, 0.8826, 1.0000],\n",
       "         [0.2665, 0.5093, 0.7056, 0.8741, 1.0000]]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cumulative_prob = torch.cumsum(sorted_prob, dim=-1)\n",
    "sorted_prob,cumulative_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ab11586c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True],\n",
       "        [True, True, True, True, True]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# если не осталось ни одного токена, то хотя бы самый вероятный оставим\n",
    "top_p = 0.2\n",
    "to_remove = cumulative_prob > top_p\n",
    "to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "95e3e7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False,  True,  True,  True,  True],\n",
       "        [False,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "to_remove[:,0]=False\n",
    "to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "218345a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[False, False,  True,  True,  True],\n",
       "        [False,  True,  True,  True,  True]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top_p = 0.5\n",
    "to_remove = cumulative_prob > top_p\n",
    "to_remove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "ae28a38a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 2, 4, 1, 3],\n",
       "         [1, 4, 0, 2, 3]]),\n",
       " tensor([[0, 3, 1, 4, 2],\n",
       "         [2, 0, 3, 4, 1]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# to_remove и sorted_prob, sorted_prob_ind индексированы одинаково\n",
    "# sorted_prob_ind указывает на элемент в исходном prob\n",
    "# построим обраный индекс, чтоб бежать по исходному prob и получать индекс в sorted_prob\n",
    "inverse_ind = torch.argsort(sorted_prob_ind, dim=-1)\n",
    "sorted_prob_ind, inverse_ind"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "409b7725",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[False,  True, False,  True,  True],\n",
       "         [ True, False,  True,  True,  True]]),\n",
       " tensor([[0.2853, 0.1977, 0.2013, 0.1174, 0.1983],\n",
       "         [0.1963, 0.2665, 0.1685, 0.1259, 0.2428]]),\n",
       " tensor([[0.9162, 0.5494, 0.5673, 0.0287, 0.5526],\n",
       "         [0.6475, 0.9534, 0.4948, 0.2036, 0.8602]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Преобразуем to_remove из отсортированного пространства в исходное\n",
    "mask = to_remove.gather(dim=-1, index=inverse_ind)\n",
    "# если теперь взять prob проб по маске, то в сумме по строке меньше top_p=0.5\n",
    "mask, prob, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f59bce3b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0.2853, 0.0000, 0.2013, 0.0000, 0.0000],\n",
       "         [0.0000, 0.2665, 0.0000, 0.0000, 0.0000]]),\n",
       " tensor([[0.9162,   -inf, 0.5673,   -inf,   -inf],\n",
       "         [  -inf, 0.9534,   -inf,   -inf,   -inf]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prob[mask]=0\n",
    "logits[mask]=float('-inf')\n",
    "prob,logits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
